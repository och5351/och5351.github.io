---
title: "Oracle 연동"
toc: true
# header:
#   image: /assets/images/spark/spark_logo.svg
#   caption: "Photo credit: [**Wikipedia**](https://upload.wikimedia.org/wikipedia/commons/f/f3/Apache_Spark_logo.svg)"
layout: posts
categories:
  - Spark
tags:
  - Spark
toc: true
toc_sticky: true

date: 2022-09-29
last_modified_at: 2022-09-29
---

<br><br>

기존의 DataWarehouse(전통적인 RDB) 에서 사용하는 테이블들을 하둡으로 백업시키는 경우가 많다.

그러나 해당 데이터들을 join하여 조회하고 싶은 경우가 발생하고, join 테이블이 이력성 테이블인 경우 데이터 변경이 자주 일어나게 됨에 따라 Hive에서 조회할 경우 정확성을 유지하기가 어렵게 된다.

그러므로 이력성 테이블들은 배치를 구성하지 않고, 메모리에서 불러와 Hive에 있는 데이터와 join 하면 된다.

<br><br>

# Spark config 설정

<br>

spark-defaults.conf 를 설정하여 준다. 그 전에 먼저 RDB Connector를 다운 받아야 한다.

CDP 의 경우 Spark 구성에서 `spark-defaults.conf` 를 검색해서 아래와 같이 넣어 준 후 클라이언트 재배포한다.

```bash
# 오라클의 경우
spark.driver.extraClassPath=/usr/lib/ojdbc8.jar
```

<br><br>

# PySpark 실행

<br>

```python
from pyspark.sql import SparkSession

spark = SparkSession \
        .builder \
        .appName("MyApp") \
        .getOrCreate()

def ora_spark(sql:str) -> pyspark.sql:
  return spark.read \
              option("url", "jdbc:oracle:thin:@IP or HOSTNAME:POTE:SID") \
              .option("user", "ID") \
              .option("password", "PW") \
              # .option("dbtable", "SERVICE_NAME.USER_INFO") \ 테이블 전체 조회하고 싶은 경우
              .option("query", sql)
              .load()

SQL = "SELECT * FROM USER.TEST_TABLE WHERE ROWNUM < 10"

ora_spark(SQL)
```
