---
title: "Ubuntu Hadoop 설치"
toc: true
# header:
# image: /assets/images/nifi/nifi_logo.svg
# caption: "Photo credit: [**Wikipedia**](https://upload.wikimedia.org/wikipedia/commons/f/ff/Apache-nifi-logo.svg)"
layout: posts
categories:
  - Setting
tags:
  - Setting
  - Hadoop
toc_sticky: true
date: 2023-03-01
last_modified_at: 2023-03-01
---

Ubuntu 에서 Hadoop 을 설치하는 과정을 기록한다.
컴퓨터 2대를 활용하여 설치하는 과정이다.
Hadoop 은 현 시점(2023.03.01) 최신 버전인 3.3.4를 설치한다.

<br><br>

# 1. 자바 설치

<br>

hadoop 3.3.4 는 jdk 11 버전을 권장하고 있으므로 `openjdk-11-jdk`를 설치한다.

<a href="https://och5351.github.io/setting/CentOS7-Java-%EC%84%A4%EC%B9%98/">CentOS7 Java 설치</a>

<br><br>

# 2. 계정 설정
<br>

```bash
# hdfs 그룹 및 계정 생성
addgroup --gid 2001 hdfs
adduser --create-home --shell /bin/bash --gid 2001 --uid 2001 hdfs
passwd hdfs

# yarn 그룹 및 계정 생성
addgroup --gid 2002 yarn
adduser --create-home --shell /bin/bash --gid 2002 --uid 2002 yarn
passwd yarn
```

<br><br>

# 3. Data directory 생성

<br>

```bash
mkdir -p /data/hdfs/namenode
mkdir -p /data/hdfs/jornalnode
chown -R hdfs:hdfs /data/hdfs
mkdir -p /data/yarn
chown -R yarn:yarn /data/yarn
```

<br><br>

# 4. Hostname 설정 및 /etc/hosts 설정

<br>

```bash
# master node setting
hostnamectl set-hostname master1.dev.com

# worker node setting
hostnamectl set-hostname worker1.dev.com

# 모든 노드 hosts 파일 변경
vi /etc/hosts 

192.168.0.2 master1.dev.com
192.168.0.3 worker1.dev.com
```

<br><br>

# 5. SSH Key 교환

namenode가 될 master 노드에 hdfs, yarn 모두 키를 생성하여 각각 노드마다 키 교환을 해준다.

<br>

```bash
# hdfs
su hdfs
ssh-keygen # 공개키 생성
ssh-copy-id hdfs@master1.dev.com
ssh-copy-id hdfs@worker1.dev.com
exit

# yarn
su yarn
ssh-keygen #공개키 생성
ssh-copy-id yarn@master1.dev.com
ssh-copy-id yarn@worker1.dev.com
exit
```

<br><br>

# 6. 하둡 다운

<br>

모든 노드에 하둡을 설치해 준다.

```bash
# /opt 경로에 다운을 받아준다.
cd /opt

wget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz

tar -zxvf hadoop-3.3.4.tar.gz

sudo mkdir /opt/hadoop-3.3.4/pids
sudo mkdir /opt/hadoop-3.3.4/logs
sudo chown -R hdfs:hdfs /opt/hadoop-3.3.4
sudo chmod 757 /opt/hadoop-3.3.4/pids
sudo chmod 757 /opt/hadoop-3.3.4/logs
```

<br><br>

# 7. 하둡 설정(master node)

<br>

1. hadoop-env.sh

```bash
# master 노드 설정
# /opt/hadoop-3.3.4/etc/hadoop 이동
cd /opt/hadoop-3.3.4/etc/hadoop

sudo vi hadoop-env.sh

# hadoop-env.sh(맨 아래에 저장)
export JAVA_HOME=$(dirname $(dirname $(readlink -f $(which java))))
export HADOOP_HOME=/opt/hadoop-3.3.4
export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop 
export HADOOP_MAPRED_HOME=${HADOOP_HOME} 
export HADOOP_COMMON_HOME=${HADOOP_HOME} 
export HADOOP_LOG_DIR=${HADOOP_HOME}/logs 
export HADOOP_PID_DIR=${HADOOP_HOME}/pids 
export HDFS_NAMENODE_USER="hdfs" 
export HDFS_DATANODE_USER="hdfs" 
export YARN_RESOURCEMANAGER_USER="yarn" 
export YARN_NODEMANAGER_USER="yarn"
```

2. core-site.xml

```bash
sudo vi core-site.xml
```

```xml
<configuration>
	<property>
		<name>fs.defaultFS</name>
		<value>hdfs://master1.dev.com:9000</value>
		<description>NameNode URI</description>
	</property>
	<property>
		<name>io.file.buffer.size</name>
		<value>131072</value> 
		<description>Buffer size</description>
	</property> <!-- HA Configuration --> 
	<property> 
		<name>ha.zookeeper.quorum</name>
		<value>zookeeper-001:2181,zookeeper-002:2181,zookeeper-003:2181</value>
	</property> 
	<property> 
		<name>dfs.ha.fencing.methods</name>
		<value>sshfence</value>
	</property>
	<property>
		<name>dfs.ha.fencing.ssh.private-key-files</name>
		<value>/home/hdfs/.ssh/id_rsa</value>
	</property>
</configuration>
```

3. hdfs-site.xml

```bash
sudo vi hdfs-site.xml
```

```xml
<configuration>
	<property>
		<name>dfs.replication</name>
		<value>1</value>
	</property>
	<property>
		<name>dfs.permissions</name>
		<value>false</value>
	</property>
	<property>
		<name>dfs.namenode.http.address</name>
		<value>master1.dev.com:9870</value>
	</property>
	<property>
		<name>dfs.namenode.name.dir</name>
		<value>file:/data/hdfs/namenode</value>
	</property>
</configuration>
```

4. mapred-site.xml

```bash
sudo vi mapred-site.xml
```

```xml
<configuration>
	<property>
		<name>mapreduce.framework.name</name>
		<value>yarn</value>
	</property>
</configuration>
```

5. yarn-site.xml

```bash
sudo vi yarn-site.xml
```

```xml
<configuration>
	<property>
		<name>yarn.nodemanager.aux-services</name>
		<value>mapreduce_shuffle</value>
	</property>
	<property>
		<name>yarn.resourcemanager.hostname</name>
		<value>master1.dev.com</value>
	</property>
	<property>
		<name>yarn.nodemanager.resource.memory-mb</name>
		<value>1024</value>
	</property>
	<property>
		<name>yarn.nodemanager.resource.cpu-vcores</name>
		<value>1</value>
	</property>
  <property>
    <name>yarn.resourcemanager.webapp.address</name>
    <value>master1.dev.com:8080</value>
  </property>
  <property>
    <name>yarn.webapp.ui2.enable</name>
    <value>true</value>
  </property>
	<property>
		<name>yarn.nodemanager.env-whitelist</name>        
		<value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
	</property>
</configuration>
```

6. workers

```bash
sudo vi workers
```

```
master1.dev.com
workder1.dev.com
```
<br><br>

# 8. 하둡 설정(worker node)

<br>

1. hadoop-env.sh

```bash
cd /opt/hadoop-3.3.4/etc/hadoop

sudo vi hadoop-env.sh

export JAVA_HOME=$(dirname $(dirname $(readlink -f $(which java))))
export HADOOP_HOME=/opt/hadoop-3.3.4
export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop 
export HADOOP_MAPRED_HOME=${HADOOP_HOME} 
export HADOOP_COMMON_HOME=${HADOOP_HOME} 
export HADOOP_LOG_DIR=${HADOOP_HOME}/logs 
export HADOOP_PID_DIR=${HADOOP_HOME}/pids 
export HDFS_NAMENODE_USER="hdfs" 
export HDFS_DATANODE_USER="hdfs" 
export YARN_RESOURCEMANAGER_USER="yarn" 
export YARN_NODEMANAGER_USER="yarn"
```

2. core-site.xml

```bash
sudo vi core-site.xml
```

```xml
<configuration>
	<property>
		<name>fs.defaultFS</name>
		<value>hdfs://master1.dev.com:9000</value>
	</property>
</configuration>
```

3. hdfs-site.xml

```bash
sudo vi hdfs-site.xml
```

```xml
<configuration>
	<property>
		<name>dfs.replication</name>
		<value>1</value>
	</property>
	<property>
		<name>dfs.permissions</name>
		<value>false</value>
	</property>
	<property>
		<name>dfs.datanode.name.dir</name>
		<value>file:/data/hdfs/datanode</value>
	</property>
</configuration>
```

4. mapred-site.xml

```bash
sudo vi mapred-site.xml
```

```xml
<configuration>
	<property>
		<name>mapreduce.framework.name</name>
		<value>yarn</value>
	</property>
</configuration>
```

5. yarn-site.xml

```bash
sudo vi yarn-site.xml
```

```xml
<configuration>
	<property>
		<name>yarn.nodemanager.aux-services</name>
		<value>mapreduce_shuffle</value>
	</property>
	<property>
		<name>yarn.resourcemanager.hostname</name>
		<value>master1.dev.com</value>
	</property>
	<property>
		<name>yarn.nodemanager.resource.memory-mb</name>
		<value>1024</value>
	</property>
	<property>
		<name>yarn.nodemanager.resource.cpu-vcores</name>
		<value>1</value>
	</property>
  <property>
    <name>yarn.resourcemanager.webapp.address</name>
    <value>master1.dev.com:8080</value>
  </property>
  <property>
    <name>yarn.webapp.ui2.enable</name>
    <value>true</value>
  </property>
	<property>
		<name>yarn.nodemanager.env-whitelist</name>        
		<value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
	</property>
</configuration>
```

<br><br>

# 9. 하둡 실행

<br>

마스터 노드에서 아래와 같이 하둡 포맷을 시킨다.

```
/opt/hadoop-3.3.4/bin/hdfs namenode -format
/opt/hadoop-3.3.4/sbin/start-all.sh
```

아래로 접속하여 ui 를 확인해본다.

* master1.dev.com:9870
* master1.dev.com:8080/ui2

<br><br>