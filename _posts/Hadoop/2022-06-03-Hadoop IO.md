---
title: "Hadoop IO(작성중)"
# header:
#   image: /assets/images/hadoop/hadoop_logo.svg
#   caption: "Photo credit: [**Unsplash**](https://unsplash.com)"
layout: posts
categories:
  - Hadoop
tags:
  - Hadoop
toc: true
toc_sticky: true

date: 2022-06-03
last_modified_at: 2022-06-03
---

<br>

하둡은 데이터 I/O를 위한 프리미티브(Primitive)를 제공한다. 데이터 무결성과 압축 같은 일부 프리미티브는 하둡보다 일반적인 기술이지만 멀티테라바이트의 데이터셋을 처리할 때는 이 프리미티브를 특별히 고려할 만한 가치가 있다. 다른 프리미티브로는 직렬화 프레임워크나 디스크 기반 데이터 구조와 같은 분산 시스템을 개발하기 위한 구성요소를 제공해주는 하둡 도구나 API가 있다.

<br><br>

# 데이터 무결성

<br>

손상된 데이터를 검출하는 일반적인 방법은 데이터가 시스템에 처음 유입되었을 때와 데이터를 손상시킬지도 모르는 신뢰할 수 없는 통신 채널로 데이터가 전송되었을 때마다 체크섬을 계산하는 것이 있다. 만일 새롭게 생성된 체크섬이 원본과 정확히 일치하지 않는다면 그 데이터는 손상된 것으로 간주한다. 이러한 기술은 데이터를 원상 복구하는 방법을 제공하지 않고 단순히 에러 검출만 수행한다. 따라서 저가의 하드웨어를 권장하지 않으며, 특히 ECC 메모리를 사용해야 한다. 주목할 점은 데이터가 아니라 체크섬이 손상될 수도 있다는 것이다. 그러나 체크섬은 데이터보다 훨씬 작기 때문에 손상될 가능성은 매우 낮다.

> 체크섬 : 중복 검사의 한 형태. 송신된 자료의 무결성을 보호하는 단순한 방법이다. 나열된 데이터를 더하여 체크섬 숫자를 얻고, 정해진 비트수의 모듈라로 정해진 비트수로 재구성 한다.

> ECC 메모리 : 오류 정정 코드 메모리(Error correction code memory). 가장 일반적인 종류의 내부 데이토 손상을 감지하고 수정하는 기억 장치의 일종이다. ECC 메모리는 계산과학, 금융 컴퓨팅 등 모든 상황에서 데이터 손상에 대처해야 하는 대부분의 컴퓨터에 사용된다.

일반적으로 에러 검출 코드는 모든 크기의 입력에 대해 32비트 정수 체크섬을 계산하는 CRC-32(32비트 순환 중복 검사 32-bit cyclic redundancy check)를 사용한다. CRC-32는 하둡의 ChdecksumFileSystem에서 체크섬 계산을 하기 위해서도 사용되지만 HDFS에서는 CRC-32C라고 불리는 매우 효율적인 변형을 사용한다.

<br><br>

# HDFS의 데이터 무결성

<br>

HDFS는 모든 데이터를 쓰는 과정에서 내부적으로 체크섬을 계산하고, 데이터를 읽는 과정에서 체크섬을 기본적으로 검증한다. dfs.bytes-per-checksum에 설정된 크기 만큼의 모든 바이트 데이터에 대해 별도의 체크섬이 생성된다. 기본적으로는 512바이트고, CRC-32C 체크섬이 4바이트의 long 타입이기 때문에 스토리지 오버헤드는 1%도 되지 않는다.

데이터노드는 데이터와 체크섬을 저장하기 전에 수신한 데이터를 검증할 책임이 있다. 이 같은 검증은 클라이언트로부터 수신한 데이터 또는 복제 과정에서 다른 데이터노드로부터 수신한 데이터에 대해 수행된다. 데이터를 쓰는 클라이언트가 데이터노드 파이프라인으로 데이터를 보내면 파이프라인의 마지막 데이터노드는 해당 데이터의 체크섬을 검증한다. 만일 데이터노드가 에러를 검출하면 클라이언트는 IOException의 서브클래스를 예외로 받고 어플리케이션 특성에 맞게 처리한다(예를 들어 재연산을 시도할 수 있다).

클라이언트가 데이터노드로부터 데이터를 읽을 때 클라이언트 역시 데이터노드에 저장된 체크섬과 수신된 데이터로부터 계산된 체크섬을 검증한다. 각 데이터노드는 체크섬 검증 로그를 영구 저장하기 때문에 각각의 블록이 검증되었던 마지막 시간을 알고 있다. 클라이언트가 성공적으로 하나의 블록을 검증하고 데이터노드에 알리면 데이터노드는 체크섬 검증에 대한 로그를 갱신한다. 이러한 통계치의 저장은 오류 디스크 검출에 유용하다.

클라이언트의 읽기 과정에서 블록을 검증하는 것 외에도 각 데이터노드는 저장된 모든 블록을 주기적으로 검증하는 DataBlockScanner를 백그라운드 스레드로 수행한다. 이것은 물리적 저장 매체에서 발생할 수도 있는 '비트로트(bit rot)'에 의한 데이터 손실을 피하기 위한 방법이다.

> 비트로트(bit rot) : 데이터 저장 장치에서 사소한 오류가 누적되어 컴퓨터 데이터가 점진적으로 손상되는 것을 이른다. 이 현상은 데이터 붕괴, 데이터 부패 또는 비트 부패라고도 일컬어진다.

HDFS는 블록의 복제본을 저장하기 때문에 손상되지 않은 새로운 복제본 생성을 위해 정상 복제본 중 하나를 복사하는 방식으로 손상된 블록을 치료할 수 있다. 만일 클라이언트가 블록을 읽는 과정에서 에러를 검출하면 훼손된 블록과 데이터노드에 대한 정보를 네임노드에 보고하고 ChecksumException을 발생시킨다. 네임노드는 그 블록 복제본이 손상되었다고 표시하고 다른 클라이언트에 제공하거나 또 다른 데이터노드로 복사하지 못하도록 한다. 그리고 네임노드는 해당 블록을 다른 데이터노드에 복제되도록 스케줄링해서 그 블록의 복제계수(replication factor)를 원래 수준을 ㅗ복구한다. 이러한 일이 발생하면 손상된 복제본은 삭제된다.

파일을 읽으려고 open() 메서드를 호출하기 전에 FileSystem의 setVerifychecksum() 메서드에 false를 전달해서 체크섬 검증을 비활성화할 수 있다. 쉘에서 -get 또는 -copyToLocal 명령어에 -ignoreCrc 옵션을 사용해도 동일하게 동작한다. 이 기능은 손상된 파일을 조사하려 할 때 이 파일로 무엇을 할 지 결정할 수 있어서 매우 유용하다. 예를 들어 해당 파일을 지우기 전에 그것을 복구할 수 있는지 여부를 파악하고자 할 수 있다.

Hadoop fs -checksum을 이용해서 파일의 체크섬을 확인할 수 있다. 이는 HDFS 안의 두 파일이 동일한 내용인지 확인할 때 유용한다. 예를 들면 distcp가 이러한 일을 수행한다.

<br><br>

# LocalFileSystem

<br>

하둡 LocalFileSystem 은 클라이언트 측 체크섬을 수행한다. filename이라는 파일을 쓸 때 파일 시스템 클라이언트는 파일과 같은 위치의 디렉터리에 그 파일의 각 청크별 체크섬이 담긴 .filename.crc라는 숨겨진 파을 내부적으로 생성한다. 청크 크기는 기본적으로 512파이트이며 file.bytes-per-checksum 속성으로 변경할 수 있다. 청크 크기는 .crc 파일에 메타데이터로 저장되기 때문에 청크 크기의 설정이 변경되다 하더라도 파일을 다시 정확하게 읽을 수 있다. 파일을 읽을 때 체크섬이 검증되고 에러가 검출되면 LocalFileSystem이 ChecksumException을 발생한다.

체크섬은 일반적으로 파일을 읽고/쓰는 시간에 몇 퍼센트의 오버헤드를 추가하는 정도이므로 전체 계산 성능에 미치는 영향이 미미하다(자바에서는 원시 코드로 구현된다.) 대부분의 애플리케이션에서 이는 데이터 무결성을 위해 납득할 만한 비용이라고 할 수 있다. 그러나 기존 파일시스템이 자체적으로 체크섬을 지원한다면 LocalFileSystem의 체크섬을 비활성화할 수도 있다. 이를 위해 LocalFileSystem 대신 RawLocalFileSystem을 사용하면 된다. 애플리케이션에 전역적으로 체크섬을 비활성화하려면 fs.file.impl 속성을 org.apache.hadoop.fs.RawLocalFileSystem 값을 설정해서 파일 URI 구현을 변경(remap) 하면 된다. 대안으로 RawLocalFileSystem 인스턴스를 직접 생성할 수도 있는데, 이는 다음 에처럼 일부 읽기에 대해서만 체크섬 검증을 비활성화할 때 유용하다.

``` java
Configuration conf = ...
FileSystem fs = new RawLoaclFileSystem();
fs.initialize(null, conf);
```

<br><br>

# ChecksumFileSystem

<br>

LocalFileSystem이 동작할 때 checksumFileSystem 을 사용하며, 이 클래스는 단순한 FileSystem의 래퍼로 구현되어 있기 때문에 다른 체크섬이 없는 파일시스템에 체크섬 기능을 쉽게 추가할 수 있다. 일반적인 사용법은 다음과 같다.

``` java
FileSystem rawFs = ...
FileSystem checksummedFs = new ChecksumFileSystem(rawFs);
```

내부의 파일시스템은 원신 파일시스템이라 불리며, ChecksumFilesystem의 getRawFileSystem() 메서드를 사용해서 얻을 수 있다.

ChecksumFileSystem 이 파일을 읽을 때 에러를 검출하면 reportChecksumFailure() 메서드를 호출할 것이다. 기본적인 구현에는 아무것도 하지 않는 것으로 되어 있지만 LocalFileSystem은 문제가 되는 파일과 체크섬을 같은 디바이스의 bad_files라는 별도의 디렉터리로 이동시킨다. 관리자는 주기적으로 이러한 훼손된 파일을 체크하고 그에 대해 적절한 처리를 해야 한다.

<br><br>

# 압축

<br>

파일 압축은 파일 저장 공간을 줄이고, 네트워크 도는 디스크로부터 데이터 전송을 고속화할 수 있는 두 가지 커다란 이점이 있다. 이러한 두 가지 이점은 대용량 데이터를 처리할 대 매우 중요하기 때문에 하둡에서 압축이 사용되는 방법을 주의 깊게 살펴봐야 한다.

다양한 특성을 가지고 있는 여러 종류의 압축 포맷, 도구, 알고리즘이 존재한다. 하둡에서 사용할 수 있는 일반적인 알고리즘이다.

<br>
<div align='center'>
<table>
  <thead>
        <tr>
          <th colspan="1">압축 포맷</th>
          <th colspan="1">도구</th>
          <th colspan="1">알고리즘</th>
          <th colspan="1">파일 확장명</th>
          <th colspan="1">분할 가능</th>
        </tr>
    </thead>
    <tbody>
        <tr>
          <td>DEFLATE(1)</td>
          <td>N/A</td>
          <td>DEFLATE</td>
          <td>.deflate</td>
          <td>No</td>
        </tr>
        <tr>
          <td>gzip</td>
          <td>gzip</td>
          <td>DEFLATE</td>
          <td>.gz</td>
          <td>No</td>
        </tr>
        <tr>
          <td>bzip2</td>
          <td>bzip2</td>
          <td>bzip2</td>
          <td>.bz2</td>
          <td>Yes</td>
        </tr>
        <tr>
          <td>LZO</td>
          <td>lzop</td>
          <td>LZO</td>
          <td>.lzo</td>
          <td>No(2)</td>
        </tr>
        <tr>
          <td>LZ4</td>
          <td>N/A</td>
          <td>LZ4</td>
          <td>.lz4</td>
          <td>No</td>
        </tr>
        <tr>
          <td>Snappy</td>
          <td>N/A</td>
          <td>Snappy</td>
          <td>.snappy</td>
          <td>No</td>
        </tr>
    </tbody>
</table>
</div>

> (1) DEFLATE는 표준 구현이 zlib인 압축 알 고리즘이다. gzip파일 포맷은 널리 이용되고 있지만 DEFLATE 포맷의 파일을 생성해주는 범용 명령행 도구는 없다. 주목할 점은 gzip 파일 표맷은 부가적인 헤더(header)와 푸터(footer)를 포함한 DEFLATE 파일 포맷이라는 것이다. .deflate 파일 확장명은 하둡의 관례다.<br>
> (2) 하지만 LZO 파일의 경우 전처리 과정에서 색인을 생성했다면 분할할 수 있다. 


모든 압축 알고리즘은 압축과 해제가 빨라질수록 공간이 늘어나는 희생을 감수해야 하기 때문에 공간과 시간은 트레이드오프 관계에 있다. 보통 9개의 옵션(-1은 스피드 최적화, -9는 공간 최적화를 의미)을 제공함으로써 어느 정도 이러한 트레이드오프에 대한 제어를 가능하게 한다. 예를 들어 다음 명령은 가장 빠른 압축 메서드를 사용해서 file.gz 라는 압축 파일을 생성한다.

``` bash
gzip -1 file
```

각 도구는 각기 다른 압축 특성이 있다. gzip은 일반적인 목적의 압축도구고, 공간/시간 트레이드오프의 중앙에 위치한다. bzip2는 gzip보다 더 효율적으로 압축하지만 대신 더 느리다. bzip2의 압축 해제 속도는 압축 속도보다 더 빠르지만 여전히 다른 포맷에 비해 더 느리다. 한편 LZO, LZ4, Snappy는 모두 속도에 최적화되었고 gzip보다 어느정도 빠르지만 압축 효율은 떨어진다. Snappy와 LZ5는 압축 해제 속도 측면에서 LZO보다 매우 빠르다.

분할 가능은 압축 포맷이 분할을 지원하는지 여부를 알려준다. 예를 들어 스트림의 특정 지점까지 탐색한 후 이후의 일부 지점으로부터 읽기를 시작할 수 있는지 알려준다. 분할 가능한 압축 포맷은 특히 맵리듀스에 적합하다.

<br><br>

# 코덱

<br>

코덱은 압축-해제 알고리즘을 구현한 것이다. 하둡에서 코덱은 CompressionCodec 인터페이스로 구현된다. 예를 들어 GzipCodec은 gzip을 위한 압축과 해제 알고리즘을 담고 있다.

<div align='center'>
  <table>
    <thead>
      <tr>
        <th colspan='1'>압축 포맷</th>
        <th colspan='1'>하둡 압축 코덱</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>DEFLATE</td>
        <td>org.apache.hadoop.io.compress.DefaultCodec</td>
      <tr>
      <tr>
        <td>Gzip</td>
        <td>org.apache.hadoop.io.compress.GzipCodec</td>
      <tr>
      <tr>
        <td>bzip2</td>
        <td>org.apache.hadoop.io.compress.Bzip2Codec</td>
      <tr>
      <tr>
        <td>LZO</td>
        <td>org.apache.hadoop.io.compress.LzopCodec</td>
      <tr>
      <tr>
        <td>LZ4</td>
        <td>org.apache.hadoop.io.compress.Lz4Codec</td>
      <tr>
      <tr>
        <td>Snappy</td>
        <td>org.apache.hadoop.io.compress.SnappyCodec</td>
      <tr>
    </tbody>
  </table>
</div>

LZO 라이브러리는 GPL 라이선스고 아파치 배포판에 포함되지 않을 수 있으므로 하둡 코덱은 <a href='http://code.google.com/p/hadoop-gpl-compression/'>http://code.google.com/p/hadoop-gpl-compression/</a> 나 <a href='http://github.com/kevinweil/hadoop-lzo'>http://github.com/kevinweil/hadoop-lzo</a>에서 별도로 내려받아야 한다. LzopCodec은 lzop 도구와 호환되고 핵심적인 LZO 포맷에 부가적인 헤더가 포함된 것으로 일반적으로 사용자가 원하는 형태다. 또한 순수한 LZO 포맷을 위한 LzoCodec이 있는데, .lzo_deflate 파일 확장명(DEFLATE로 유축해봤을 때 헤더 없는 gzip 포맷이다)을 사용한다.

<br><br>

# CompressionCodec 을 통한 압축 및 해제 스트림

<br>

CompressionCodec은 데이터를 쉽게 압축하거나 해제해주는 두 개의 메서드를 제공한다. 출력 스트림에 쓸 데이터를 압축하려면 createOutputStream(OutputStreamout out) 메서드를 사용하는데, 이 메서드는 압축되지 않은 데이터를 압축된 형태로 내부 스트림에 쓰는 CompressionOutputStream을 생성한다 반대로 입력 스트림으로부터 읽어 들인 데이터를 압축 해제하려면 createInputStream(InputStream in) 메서드를 호출하는데, 미 메서드는 기존 스트림으로부터 비압축 데이터를 읽을 수 있는 CompressioninputStream을 반환한다.

CompressionOutputStream과 CompressionInputStream은 기존 압축기 또는 압축 해제기를 재설정할 수 잇는 기능을 제공하는 것을 제외하면 각가 java.util.zip.DeflaterOutputStream, java.util.zip.DeflaterInputStream과 비슷하다. SequenceFile과 같이 데이터 스트림의 구간을 별개의 블록으로 압축하는 애플리케이션에서 사용되는 중요한 메서드다.

``` java
public class StreamCompressor {
  public static void main(String[] args) throw Exception {
    String codecClassname = args[0];
    Class<?> codeClass = Class.forName(codecClassname);
    Configuration conf = new Configuration();
    CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, conf);

    CompressionOutputStream out = codec.createOutputStream(System.out);
    IOUtils.copyBytes(System.in, out, 4096, false);
    out.finish();
  }
}
```

애플리케이션의 첫 번째 명령행 인자는 CompressionCodec 패키지의 정규화된 전체 이름이다. 그 코덱의 새로운 인스턴스를 생성하기 위해 RevflectionUtils를 사용하고 System.out 의 압축 래퍼를 얻는다. 그리고 입력을 출력으로 복사하기 위해 IOUtils의 copyBytes() 유틸리티 메서드를 호출하면 CompressionOutputStream으로 압축한다. 마지막으로 CompressionOutputStream의 finish()를 호출하고, 압축 도구에 압축된 스트림 쓰기 중단을 요청하지만 스트림 자체를 닫지는 않는다. 이를 다음과 같이 명령행에서 시험해볼 수 있는데, 이 예제는 GzipCodec으로 StreamCompressor를 사용해서 'Text' 문자열을 압축하고 gunzip을 사용하여 표준 입력으로부터 압축 해제 한다.

``` bash
echo "Text" | hadoop StreamCompressor org.apache.hadoop.io.copmress.GzipCodec | gunzip -
```

<br><br>

# CompressionCodecFactory를 사용하여 CompressionCodec 유추하기

<br>

압축된 파일을 읽을 때 일반적으로 해당 파일 확장명을 보면 사용된 코덱을 유축할 수 있다 예를 들어 .gz 로 끝나는 파일은 GzipCodec으로 읽을 수 있다.

CompressionCodecFactory의 getCodec() 메소드는 지정된 파일에 대한 Path 객체를 인자로 받아 파일 확장명에 맞는 CompressionCodec을 찾아준다.

``` java
public class FileDecompressor {
  public static void main(String[] args) throws Exception {
    String uri = args[0];
    Configuration conf = new Configuration();
    FileSystem fs = FileSystem.get(URI.create(uri), conf);

    Path inputPath = new Path(uri);
    CompressionCodecFactory factory = new CompressionCodecFactory(conf);
    CompressionCodec codec = factory.getCodec(inputPath);
    if(codec == null){
      System.err.println("No codec found for " + uri);
      System.exit(1);
    }

    String outputUri = CompressionCodecFactory.removeSuffix(uri, codec.getDefaultExtension());

    InputStream in = null;
    OutputStream out = null;

    try{
      in = codec.createInputStream(fs.open(inputPath));
      out = fs.create(new Path(outputUri));
      IOUtils.copyBytes(in, out, conf);
    } finally {
      IOUtils.closeStream(in);
      IOUtils.closeStream(out);
    }
  }
}
```
일단 코덱을 찾았으면 출력 파일의 이름을 생성하기 위해 CompressionCodecFactory의 removeSuffix() 정적 메서드로 파일 접미사를 제거한다. 따라서 다음처럼 프로그램을 호출하면 file.gz로 명명된 파일은 file로 압축 해제된다.

```
hadoop FileDecompressor file.gz
```

CompressionCodecFacgtory는 LZO 제외 코덱과 io.compression.codecs 속성에 나열된 코덱을 불러온다. 기본적으로 io.compression.codecs 속성은 비어 있다. 따라서 새롭게 등록하는 커스텀 코덱(외부 로딩이 필요한 LZO 코덱과 같은)을 얻고자 할 때만 환경 속성을 변경할 필요가 있다. 각 코덱은 기본 파일 확장명을 알고 있으므로 CompressionCodecFactory가 주어진 확장과 일치하는 코덱을 찾기 위해 등록된 코덱 전체를 검색한다.

<br>

<div align='center'>
  <table>
    <thead>
      <tr>
        <th colspan='1'>속성명</th>
        <th colspan='1'>타입</th>
        <th colspan='1'>기본값</th>
        <th colspan='1'>설명</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>io.compression.codecs</td>
        <td>콤마로 구분된 클래스 이름</td>
        <td></td>
        <td>압축 및 해제를 위해 추가하고자 하는 CompressionCodec 클래스 목록</td>
      </tr>
    </tbody>
</table>
</div>

<br><br>

# 원시 라이브러리

<br>

성능 관점에서 압축과 해제를 위해 원시 라이브러리를 사용하는 것이 바람직하다. 예를 들어 원시 gzip 라이브러리를 사용하여 테스트해본 결과 압축 해제 성능은 최대 50%, 압축 성능은 거의 최대 10% 정도 더 좋아졌다(내장된 자바 구현과 비교하여). 각 압축 포맷에 대한 자바와 원시 구현제의 사용 가능 여부를 보여준다. 모든 포맷은 원시 구현체를 가지고 있지만 모든 포맷이 자바 구현체를 가지고 있는 것은 아니다(ex. LZO).

<br>

<table>
  <thead>
    <tr>
      <th colspan='1'>DEFLATE</th>  
    </tr>
  </thead>
</table>