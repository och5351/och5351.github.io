---
title: "데이터 흐름"
header:
  image: /assets/images/hadoop/hadoop_logo.svg
  caption: "Photo credit: [**Unsplash**](https://unsplash.com)"
layout: posts
categories:
  - Hadoop
tags:
  - Hadoop
  - CLI
toc: true
toc_sticky: true

date: 2022-05-02
last_modified_at: 2022-05-05
---

<a id="home1"></a>
<br><br>

# 파일 읽기 상세

<br>

클라이언트가 HDFS, Namenode, Datanode 와 어떻게 상호작용하는지 데이터 흐름 관점에서 이해하기 위해 파일읽기 이벤트의 기본 순서

<br>

<div align='center'>
<img src='https://user-images.githubusercontent.com/45858414/166908392-99eeb749-b8fe-4bf7-a0c6-317badca509f.png' height ='70%', weight='70%' />
</div>

<br>

1. 클라이언트는 HDFS가 DistributedFileSystem 인스턴스인 FileSystem 객체의 open() 메소드를 호출하여 원하는 파일을 연다.
2. DistributedFileSystem 은 파일의 첫 번째 블록 위치를 파악하기 위해 RPC를 사용하여 네임노드를 호출한다. 네임노드는 블록별로 해당 블록의 복제본을 가진 데이터 노드의 주소를 반환한다. 이 때 클러스터의 네트워크 위상에 따라 클라이언트와 가까운 순으로 데이터노드(예를 들면 맵리듀스 태스크)고 해당 블록의 복제본을 가지고 있으면 클라이언트는 로컬 데이터 노드에서 데이터를 읽는다.
3. DistributedFileSystem은 클라이언트가 데이터를 읽을 수 있도록 FSDataInputStream을 반환한다. FSDataInputSTream은 데이터노드와 네임노드의 I/O를 관리하는 DFSinputStream을 래핑한다. 클라이언트는 스트림을 읽기 위해 read() 메서드를 호출한다.
4. 파일의 첫 번째 블록의 데이터노드 주소를 저장하고 있는 DFSInputStream은 가장 가까운(첫번째) 데이터노드와 연결한다. 해당 스트림에 대해 read() 메소드를 반복적으로 호출하면 데이터노드에서 클라이언트로 모든 데이터가 전송된다.
5. 블록의 끝에 도달하면 DFSInputStream은 데이터노드의 연결을 닫고 다음 블록의 데이터노드를 찾는다. 클라이언트의 관점에서 이러한 과정은 투명하게 전개되며 클라이언트는 단지 연속적인 스트림을 읽는 것 처럼 느낀다.
6. 클라이언트는 스트림을 통해 블록을 순서대로 하나씩 읽는다. 이때 DFSInputStream은 블록마다 데이터노드와 새로운 연결을 맺는다. 클라이언트는 다음 블록의 데이터노드 위치를 얻기 위해 네임노드를 호출한다. 모든 블록에 대한 읽기가 끝나면 클라이언트는 FSDataInputStream의 Close 메소드를 호출한다.

<br>

데이터를 읽는 중에 데이터노드와의 통신에 장애가 발생하면 DFSInputStream은 해당 블록을 저장하고 있는 다른 데이터노드와 연결을 시도한다. 또한 이후 블록에 대한 불필요한 재시도를 방지하기 위해 장애가 발생한 데이터노드를 기억해둔다. DFSInputStream은 다른 데이터노드에 있는 블록의 복제본을 읽으려 시도한다. 몰론 손상된 블록에 대한 정보는 네임노드에 보고된다. 물론 손상된 블록에 대한 정보는 네임노드에 보고된다.

<br>

설계의 핵심으로 클라이언트는 데이터를 얻기 위해 데이터노드에 직접적으로 접촉하고, 네임노드는 각 블록에 적합한 데이터노드를 안내해준다는 것이다. 데이터 트래픽은 클러스터에 있는 모든 데이터노드에 고르게 분산되므로 HDFS는 동시에 실행되는 클라이언트의 수를 크게 늘릴 수 있다. 한 편 네임노드는 효율적인 서비스를 위해 메타데이터를 메모리에 저장하고 단순히 블록의 위치 정보 요청만 처리하며, 데이터를 저장하거나 전송하는 역할은 맡지 않으므로 클라이언트가 많아져도 병목현상은 거의 발생하지 않는다.

> 네트워크 토폴로지와 Hadoop

<br>

대용량 데이터 처리 맥락에서 노드 간 데이터 전송의 제약 요소는 전송률이다. 대역폭은 한정 자원이다. 그래서 노드 간의 네트워크 대역폭을 거리 측정에 이용한다.

노드 간의 대역폭을 실제 측정하는 것은 어렵다. 클러스터는 고정적이어야 하고 클러스터의 노드 쌍의 수는 전체 노드 수의 제곱에 이르기 때문이다. 대신 하둡은 단순한 방식을 취하는데, 네트워크 전체를 하나의 트리로 표현하고 두 노드의 거리는 가장 가까운 공통 조상과의 거리를 합산하여 계산한다. 트리의 단계는 고정되어 있지 않지만, 일반적으로 데이터 센터, 랙, 토드 순으로 구성된다. 가용 대역폭은 다은에 나오는 시나리오 순으로 점차 줄어든다고 가정한다.

- 동일 노드
- 동일 랙의 다른 노드
- 동일 데이터 센터에 있는 다른 랙의 노드
- 다른 데이터 센터에 있는 노드

예를 들어 d1 데이터 센터의 r1 랙에 있는 n1 노드를 상상해보자. 이 노드는 /d1/r1/n1으로 표시될 수 있으며, 앞서 언급한 네 개의 시나리오로 그 거리르 계산해본다.

<br>

- distance(/d1/r1/n1, /d1/r1/n1) = 0 (동일 노드)
