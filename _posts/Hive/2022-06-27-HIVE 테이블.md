---
title: "Hive 테이블"
# header:
#   image: /assets/images/hadoop/hadoop_logo.svg
#   caption: "Photo credit: [**Unsplash**](https://unsplash.com)"
layout: posts
categories:
  - Hive
tags:
  - Hadoop
  - Hive
toc: true
toc_sticky: true

date: 2022-06-27
last_modified_at: 2022-06-27
---

<br>

하이브 테이블은 `저장된 데이터`와 `테이블에서 데이터의 LAYOUT을 기술하는 관련 메타데이터`로 논리적으로 구성된다. 데이터는 로컬 파일시스템이나 S3를 포함하여 어떠한 하둡 파일 시스템에도 둘 수 있지만 일반적으로 HDFS에 둔다. 하이브는 HDFS가 아닌 관계형 데이터베이스에 메타데이터를 저장한다.

<br>

> 다중 데이터베이스/스키마 지원 <br><br>
많은 관계형 데이터베이스는 사용자와 어플리케이션이 다수의 데이터베이스나 스키마로 분리될 수 있도록 다중 네임스페이스 기능을 지원한다. 하이브 역시 다중 네임스페이스 기능을 지원하고 CREATE DATABASE dbname, USE dbname, DROP DATABASE dbname과 같은 명령을 제공한다. 데이터베이스를 포함한 테이블 이름을 사용하고 싶으면 dbname.tablename 형식으로 지정하면 된다. 데이터베이스를 지정하지 않으면 테이블은 default 데이터베이스에 포함된다.

<br><br>

# 관리 테이블과 외부 테이블

테이블을 생성할 때 하이브는 기본적으로 데이터를 직접 관리하게 되는데, 이는 하이브가 데이터를 자신이 관리하는 웨어하우스 디렉터리로 이동시킨다는 의미다. 반대로 사용자는 외부테이블을 생성하여 웨어하우스 디렉터리 외부의 데이터를 참조할 수 도 있다.

두 종류의 테이블은 LOAD와 DROP에 대한 해석에서 차이점이 있다. 사용자가 데이터를 관리 테이블에 로드할 때 그 데이터는 하이브의 웨어하우스 디렉터리로 이동하게 된다.

```SQL
CREATE TABLE managed_table (dummy STRING);
LOAD DATA INPAYH '/usr/tom/data.txt' INTO TABLE managed_table;
```

> LOAD 연산은 파일시스템에서 파일을 단순히 이동시키거나 이름을 변경하는 것이기 때문에 매우 빠르다. 하지만 하이브는 해당 테이블의 디렉터리에 잇는 파일과 관리 테이블을 위해 선언된 스키마의 정합성을 미리 점검하지 않는다는 사실을 잊어서는 안된다. 만일 정합성에 문제가 있다면 쿼리 시점에 누락된 필드에 대해 NULL을 반환한다. 사용자는 테이블에서 몇 개의 행을 얻을 수 있는 SELECT 구문을 실행하여 데이터가 제대로 해석되는지 검증할 수 있다.

``` SQL
DROP TABLE managed_table;
```

DROP 구문으로 테이블을 제거하면 메타스토어와 함께 그 데이터도 삭제된다. LOAD 구문은 파일의 이동 조작 수행, DROP은 파일의 삭제 조작을 수행한 것이기 때문에 데이터는 어디에도 존재하지 않는다.

외부 테이블은 관리 테이블과 다르며 사용자가 데이터의 생성과 삭제를 직접 제어해야 한다. 외부 데이터의 위치는 테이블을 생성하는 시점에 지정된다.

```SQL
CREATE EXTERNAL TABLE external_table (dummy STRING)
LOCATION '/usr/tom/external_table';
LOAD DATA INPATH '/usr/tom/data.txt' INTO TABLE external_table;
```

테이블을 생성할 때 EXTERNAL 키워드를 추가하면 하이브는 데이터를 직접 관리할 필요가 없기 때문에 그 데이터를 웨어하우스 디렉터리로 이동시키지 않는다. 사실 하이브는 테이블을 선언하는 시점에 외부 테이블의 디렉터리가 존재하는지 여부도 확인하지 않는다. 이렇게 하면 테이블을 생성한 후 데이터를 생성해도 문제가 없다.

외부 테이블을 삭제할 때 하이브는 데이터는 절대 건드리지 않고 메타데이터만 삭제한다.

따라서 어떤 종류의 테이블을 생성할지 선택할 필요가 있다. drop을 해석하는 방법은 다르지만 두 종류의 테이블은 큰 차이가 없기 때문에 이것은 선호의 문제가 된다. 하이브에서만 데이터를 처리한다면 관리 테이블을, 동일한 데이터셋을 하이브를 비롯해서 다른 도구와 함께 사용한다면 외부 테이블을 주로 선택한다. <b><u>일반적으로 HDFS에 다른 프로세스가 생성하여 저장한 초기 데이터셋을 접근할 때는 외부 테이블을 사용하고, 나중에 그 데이터를 하이브 관리 테이블로 이동시킬 때는 하이브 변환을 사용할 수 있다.</u></b> 데이터를 다른 어플리케이션에서 사용할 수 있도록 익스포트할 때 외부 테이블을 사용할 수도 있다.

<b><u>외부 테이블을 사용하는 또 다른 이유는 동일한 데이터셋관 관련된 다중 스키마를 적용하기 위해서다.</u></b>

<br><br>

# 파티션과 버킷

하이브는 테이블을 파티션으로 구조화할 수 있다. 파티션이란 테이블의 데이터를 날짜와 같은 파티션 컬럼의 값을 기반으로 큰 단위로 분할하는 방식이다. 파티션을 사용하면 데이터의 일부를 매우 빠르게 질의할 수 있다.

테이블과 파티션은 효율적인 쿼리를 위해 데이터에 추가된 구조인 버킷으로 더욱 세분화될 수 있다. 예를 들어 사용자 ID를 기준으로 버킷을 생성하면 전체 사용자 중에서 무작위 데이터 샘플을 뽑아 사용자가 작성한 쿼리가 제대로 실행되는지 빠르게 평가할 수 있다.

<br>

## 파티션

파티션이 주로 사용되는 예를 들기 위해 각 레코드가 타임스탬프를 포함하고 있는 로그파일이 있다고 가정. 만약 날짜 기반으로 파티션을 했다면 같은 날짜의 레코드는 동일한 파티션에 저장된다. 특정 날짜나 기간으로 국한된 쿼리는 오직 관련된 파티션의 파일만 읽기 때문에 매우 효율적이다.

테이블은 다중 차원으로 파티션될 수 있다. 예를 들어 먼저 날짜를 기준으로 로그를 파티션하고 그 다음에 지역별로 효율적인 쿼리를 수행하기 위해 각 날짜별 파티션에 국가별 서브파티션을 추가할 수 있다.

```SQL
CREATE TABLE logs (ts BIGINT, line STRING)
PARTITIONED BY (dt STRING, country STRING);
```

```SQL
LOAD DATA LOCAL INPATH 'hdfs path'
INTO TABLE logs
PARTITION (dt='2001-01-01', country='GB');
```

<br>

디렉터리 구조
```
/usr/hive/warehouse/logs
|- dt=2001-01-01/
|   |- country=GB/
|   |   |- file1
|   |   L  file2
|   L  country=US/
|       L  fil3
L dt=2001-01-02/
    |- country=GB/
    |   L  file4
    L  country=US/
        |- file5
        L  file6
```

<br>

SHOW PARTITIONS 로 테이블에 포함된 파티션의 목록을 하이브에 요청할 수 있다.

```sql
SHOW PARTITIONS logs;
```

PARTITIONED BY 절의 컬럼 정의는 파티션 컬럼이라 불리는 온전한 테이블 컬럼이라는 점을 염두에 두어야 한다. 파티션 컬럼에 대한 값은 디렉터리 이름으로 유추할 수 있기 때문에 데이터 파일 자체에는 포함되어 있지 않다.

일반적으로 파티션 컬럼은 SELECT 문에서 사용될 수 있다. 하이브는 해당 파티션만 읽기 위해 입력 가지치기를 수행한다. 

```SQL
SEELCT ts, dt, line
FROM logs
WHERE country='GB';
```

<br><br>

## 버킷

테이블을 버킷으로 구조화하려는 이유는 2가지

- 매우 효율적인 쿼리가 가능하다.

버킷팅은 테이블에 대한 추가 구조를 부여하고, 하이브는 어떤 쿼리를 수행할 때 이 추가 구조를 이용할 수 있다. 특히 동일한 컬럼(조인할 컬럼)에 대한 버킷을 가진 두 테이블을 조인할 때 맵 조인을 구현하면 매우 효율적이다.)

- 효율적인 샘플링에 유리하다.

매우 큰 데이터셋을 대상으로 개발하거나 개선하는 과정에서 데이터셋의 일부만으로 쿼리를 수행할 수 있으면 매우 편리하다.

테이블 버킷팅 방법(CLUSTERED BY)

```SQL
-- 사용자 ID 기준으로 버킷팅
CREATE TABLE bucketed_users (id INT, name STRING)
CLUSTERED BY (id) INTO 4 BUCKETS;
```

하이브는 사용자 ID의 값을 해싱하고, 그 값을 버킷 수로 나눈 나머지를 버킷의 번호로 선택한다. 그러면 각 버킷은 사용자의 무작위 집합을 포함하는 효과가 있다.

동일한 방식으로 버킷된 두 개의 테이블에 맵 조인을 수행하면 왼쪽 테이블의 버킷을 처리하는 매퍼는 상응하는 행이 오른쪽 테이블의 버킷에 있다는 것을 사전에 인지하고 있기 때문에 조인을 수행할 때 오른쪽 테이블에 저장된 전체 데이터의 작은 일부만 추출한다. 두 테이블의 버킷 수가 정확하게 같으면 좋겠지만 서로 달라도 최적화가 가능하다. 

버킷에 포함된 데이터는 하나 이상의 컬럼으로 정렬될 수 있다. 이렇게 하면 각 버킷을 조인할 때 효율적인 병합 정렬이 가능하므로 맵 조인의 속도를 더 향상시킬 수 있다. 

```SQL
-- 정렬 버킷
CREATE TABLE buketed_users (id INT, name STRING)
CLUSTERED BY (id) SORTED BY (id ASC) INTO 4 BUCKETS;
```

> 하이브는 디스크에 저장된 데이터 파일에 있는 버킷이 테이블 정의(버킷 컬럼이나 개수)에 기반한 버킷과 정확히 일치하는지 검증하지 않는다. 일치하지 않으면 에러가 발생하거나 쿼리 시점에 예측하지 못한 결과가 발생할 수 있다. 이러한 이유로 하이브가 직접 버킷팅을 수행하도록 권고하고 있다.

기존의 테이블을 버킷된 테이블로 변경하려면 먼저 `hive.enforce.bucketing` 속성을 `true`로 설정하여 테이블 정의에 선언된 개수만큼 버킷을 생성하도록 하이브에 알려주어야 한다. 이 후 버킷 테이블로 Insert 한다.

```sql
INSERT OVERWRITE TABLE bucketed_users
SELECT * FROM users;
```

물리적으로 각 버킷은 테이블이나 파티션 디렉터리 안에 있는 하나의 파일이다. 파일의 이름은 중요하지 않지만 사전순으로 배치될 때 버킷 n은 n번째 파일일다. 사실 버킷은 맵리듀스 출력 파일 파티션에 상응한다. job은 reduce 태스크와 동일한 개수의 버킷(출력 파일)을 생성한다. 

```
dfs -ls /usr/hive/warehouse/bucketed_users;
```
```
000000_0
000001_0
000002_0
000003_0
```

첫 번째 버킷은 사용자 ID가 0과 4인 레코드를 가진다. INT의 해시는 정수 그 자체고, 버킷 수인 4로 나눈 나머지가 바로 버킷의 번호다.

전체 테이블이 아니라 테이블의 일부 버킷만 사용하는 쿼리롤 제한하는 TABLESAMPLE 절을 이용하여 해당 테이블을 샘플링하면 동일한 결과를 얻을 수 있다.

```SQL
SELECT * FROM bucketed_users
TABLESAMPLE(BUKET 1 OUT OF 4 ON id);
```

여기서 버킷 번호는 1이 기준이다. 따라서 이 쿼리는 4 개의 버킷 중 첫 번째 버킷의 사용자만 검색한다. 데이터가 고르게 분포된 매우 큰 데이터셋에서는 테이블에 있는 전체 행의 4분의 1가량이 반환될 것이다. 비율을 다르게 지정하면 더 많은 버킷도 샘플링할 수 있다. 어차피 샘플링 자체가 정밀한 연산을 의미하지 않기 때문에 버킷 수를 엄격하게 지정할 필요는 없다. 예를 들어 다음 쿼리는 버킷의 절반(1/2)을 반환한다.

```sql
SELECT * FROM bucketed_users
TABLESAMPLE(BUCKET 1 OUT OF 2 ON id);
```

버킷된 테이블의 샘플링은 TABLESAMPLE 절에 해당되는 버킷만 읽으면 되기 때문에 매우 효율적이다. 버킷이 없는 테이블을 RAND() 함수로 샘플링할 때와 한 번 비교해본다면 매우 작은 샘플만 필요하지만 전체 입력 데이터셋을 읽어야하는 낭비가 생긴다.

```SQL
SELECT * FROM users
TABLESAMPLE(BUCKET 1 OUT OF 4 ON rand());
```